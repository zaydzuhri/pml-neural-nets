{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmEgiRSMmzrj"
      },
      "source": [
        "# Import module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgojAfLUfXOL"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from tabulate import tabulate\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWMSId_am7IN"
      },
      "source": [
        "# Import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_jGtiqSgi0K",
        "outputId": "823cabc9-989c-4d82-fa07-12c95f34dd8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DD9rZnpGBFsH6G2bz2CZIfHa5S1Hk1Qn\n",
            "To: /content/panda-or-bear-image-classification.zip\n",
            "100%|██████████| 12.3M/12.3M [00:00<00:00, 74.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PA1YQhlDirnBpMuwo9YZ_DKxpAcWVT-P\n",
            "To: /content/Train_stock_market.csv\n",
            "100%|██████████| 762k/762k [00:00<00:00, 30.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Ezi3W4xiRo0c3mPKV6ixtjqSLVx6jLTq\n",
            "To: /content/Test_stock_market.csv\n",
            "100%|██████████| 2.46k/2.46k [00:00<00:00, 7.68MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Jbah9KFf5iKE5w0u1B_GAY0ZTCAsXuZu\n",
            "To: /content/model_spek_cnn.json\n",
            "100%|██████████| 159k/159k [00:00<00:00, 23.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jDRZoXj6jvD_-7v32ieBnW-nfh-L6SOz\n",
            "To: /content/model_spek_rnn.json\n",
            "100%|██████████| 774k/774k [00:00<00:00, 35.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  panda-or-bear-image-classification.zip\n",
            "replace PandasBears/Test/Bears/251.jpeg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: PandasBears/Test/Bears/251.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/252.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/253.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/254.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/255.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/256.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/257.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/258.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/259.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/260.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/261.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/262.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/263.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/264.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/265.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/266.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/267.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/268.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/269.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/270.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/271.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/272.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/273.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/274.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/275.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/276.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/277.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/278.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/279.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/280.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/281.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/282.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/283.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/284.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/285.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/286.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/287.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/288.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/289.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/290.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/291.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/292.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/293.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/294.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/295.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/296.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/297.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/298.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/299.jpeg  \n",
            "  inflating: PandasBears/Test/Bears/300.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/251.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/252.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/253.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/254.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/255.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/256.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/257.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/258.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/259.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/260.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/261.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/262.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/263.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/264.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/265.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/266.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/267.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/268.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/269.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/270.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/271.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/272.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/273.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/274.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/275.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/276.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/277.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/278.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/279.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/280.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/281.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/282.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/283.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/284.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/285.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/286.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/287.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/288.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/289.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/290.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/291.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/292.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/293.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/294.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/295.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/296.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/297.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/298.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/299.jpeg  \n",
            "  inflating: PandasBears/Test/Pandas/300.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/1.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/10.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/100.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/101.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/102.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/103.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/104.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/105.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/106.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/107.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/108.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/109.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/11.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/110.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/111.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/112.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/113.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/114.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/115.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/116.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/117.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/118.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/119.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/12.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/120.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/121.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/122.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/123.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/124.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/125.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/126.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/127.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/128.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/129.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/13.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/130.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/131.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/132.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/133.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/134.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/135.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/136.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/137.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/138.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/139.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/14.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/140.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/141.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/142.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/143.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/144.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/145.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/146.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/147.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/148.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/149.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/15.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/150.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/151.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/152.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/153.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/154.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/155.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/156.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/157.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/158.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/159.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/16.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/160.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/161.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/162.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/163.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/164.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/165.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/166.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/167.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/168.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/169.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/17.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/170.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/171.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/172.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/173.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/174.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/175.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/176.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/177.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/178.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/179.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/18.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/180.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/181.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/182.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/183.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/184.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/185.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/186.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/187.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/188.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/189.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/19.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/190.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/191.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/192.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/193.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/194.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/195.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/196.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/197.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/198.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/199.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/2.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/20.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/200.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/201.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/202.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/203.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/204.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/205.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/206.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/207.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/208.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/209.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/21.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/210.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/211.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/212.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/213.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/214.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/215.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/216.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/217.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/218.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/219.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/22.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/220.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/221.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/222.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/223.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/224.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/225.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/226.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/227.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/228.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/229.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/23.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/230.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/231.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/232.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/233.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/234.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/235.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/236.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/237.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/238.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/239.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/24.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/240.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/241.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/242.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/243.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/244.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/245.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/246.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/247.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/248.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/249.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/25.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/250.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/26.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/27.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/28.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/29.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/3.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/30.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/31.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/32.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/33.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/34.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/35.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/36.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/37.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/38.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/39.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/4.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/40.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/41.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/42.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/43.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/44.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/45.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/46.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/47.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/48.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/49.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/5.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/50.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/51.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/52.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/53.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/54.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/55.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/56.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/57.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/58.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/59.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/6.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/60.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/61.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/62.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/63.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/64.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/65.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/66.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/67.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/68.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/69.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/7.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/70.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/71.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/72.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/73.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/74.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/75.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/76.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/77.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/78.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/79.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/8.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/80.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/81.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/82.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/83.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/84.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/85.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/86.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/87.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/88.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/89.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/9.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/90.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/91.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/92.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/93.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/94.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/95.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/96.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/97.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/98.jpeg  \n",
            "  inflating: PandasBears/Train/Bears/99.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/1.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/10.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/100.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/101.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/102.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/103.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/104.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/105.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/106.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/107.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/108.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/109.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/11.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/110.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/111.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/112.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/113.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/114.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/115.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/116.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/117.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/118.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/119.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/12.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/120.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/121.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/122.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/123.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/124.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/125.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/126.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/127.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/128.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/129.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/13.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/130.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/131.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/132.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/133.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/134.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/135.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/136.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/137.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/138.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/139.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/14.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/140.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/141.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/142.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/143.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/144.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/145.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/146.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/147.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/148.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/149.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/15.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/150.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/151.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/152.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/153.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/154.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/155.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/156.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/157.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/158.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/159.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/16.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/160.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/161.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/162.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/163.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/164.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/165.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/166.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/167.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/168.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/169.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/17.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/170.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/171.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/172.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/173.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/174.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/175.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/176.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/177.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/178.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/179.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/18.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/180.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/181.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/182.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/183.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/184.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/185.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/186.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/187.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/188.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/189.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/19.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/190.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/191.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/192.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/193.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/194.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/195.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/196.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/197.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/198.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/199.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/2.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/20.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/200.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/201.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/202.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/203.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/204.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/205.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/206.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/207.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/208.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/209.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/21.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/210.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/211.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/212.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/213.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/214.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/215.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/216.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/217.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/218.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/219.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/22.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/220.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/221.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/222.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/223.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/224.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/225.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/226.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/227.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/228.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/229.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/23.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/230.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/231.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/232.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/233.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/234.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/235.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/236.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/237.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/238.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/239.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/24.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/240.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/241.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/242.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/243.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/244.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/245.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/246.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/247.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/248.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/249.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/25.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/250.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/26.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/27.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/28.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/29.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/3.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/30.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/31.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/32.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/33.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/34.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/35.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/36.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/37.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/38.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/39.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/4.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/40.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/41.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/42.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/43.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/44.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/45.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/46.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/47.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/48.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/49.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/5.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/50.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/51.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/52.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/53.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/54.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/55.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/56.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/57.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/58.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/59.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/6.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/60.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/61.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/62.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/63.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/64.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/65.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/66.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/67.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/68.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/69.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/7.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/70.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/71.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/72.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/73.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/74.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/75.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/76.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/77.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/78.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/79.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/8.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/80.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/81.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/82.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/83.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/84.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/85.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/86.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/87.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/88.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/89.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/9.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/90.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/91.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/92.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/93.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/94.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/95.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/96.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/97.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/98.jpeg  \n",
            "  inflating: PandasBears/Train/Pandas/99.jpeg  \n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "import gdown\n",
        "d = {\n",
        "    'panda-or-bear-image-classification.zip':'https://drive.google.com/file/d/1DD9rZnpGBFsH6G2bz2CZIfHa5S1Hk1Qn/view?usp=sharing',\n",
        "    'Train_stock_market.csv': 'https://drive.google.com/file/d/1PA1YQhlDirnBpMuwo9YZ_DKxpAcWVT-P/view?usp=sharing',\n",
        "    'Test_stock_market.csv': 'https://drive.google.com/file/d/1Ezi3W4xiRo0c3mPKV6ixtjqSLVx6jLTq/view?usp=drive_link',\n",
        "    'model_spek_cnn.json': 'https://drive.google.com/file/d/1Jbah9KFf5iKE5w0u1B_GAY0ZTCAsXuZu/view?usp=drive_link',\n",
        "    'model_spek_rnn.json': 'https://drive.google.com/file/d/1jDRZoXj6jvD_-7v32ieBnW-nfh-L6SOz/view?usp=drive_link',\n",
        "}\n",
        "for k,v in d.items():\n",
        "  gdown.download(v, k, quiet=False, fuzzy=True)\n",
        "!unzip panda-or-bear-image-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfozfb8Tm-aC"
      },
      "source": [
        "# Core program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta5G9NMqJ4U6"
      },
      "outputs": [],
      "source": [
        "# Base class for layer-able models\n",
        "class Layer(ABC):\n",
        "    name = ''\n",
        "    def compile(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def _update_batch(self, batch_size):\n",
        "        if not hasattr(self, 'in_size') or len(self.in_size) != 4:\n",
        "            return\n",
        "        self.in_size = (batch_size, *self.in_size[1:])\n",
        "        self.out_size = self.get_out_size(self.in_size)\n",
        "\n",
        "    def backward(self, dLdy, lr):\n",
        "        pass\n",
        "\n",
        "    def get_out_size(self, in_size):\n",
        "        return in_size\n",
        "\n",
        "    @classmethod\n",
        "    @abstractmethod\n",
        "    def _deserialize(cls, data):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def _serialize(self) -> list[dict]:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IYZNwUxneBJ"
      },
      "source": [
        "## A. Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n34gJqMLwohR"
      },
      "outputs": [],
      "source": [
        "class Activation(Layer):\n",
        "    name = ''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return self.calc(x)\n",
        "\n",
        "    @abstractmethod\n",
        "    def calc(self, x):\n",
        "        pass\n",
        "\n",
        "    @classmethod\n",
        "    def _deserialize(cls, data):\n",
        "        return cls()\n",
        "\n",
        "    def _serialize(self):\n",
        "        return [{'type': self.name}]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3SIrTt_n4Zk"
      },
      "source": [
        "### 1. ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1lrR6zmfXOO"
      },
      "outputs": [],
      "source": [
        "class ReLU(Activation):\n",
        "    name = 'relu'\n",
        "\n",
        "    def calc(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def backward(self, dLdy, lr):\n",
        "        # dLdy is dLdx from next layer\n",
        "        # dLdx = dLdy * dydx (i.e. dLdinput)\n",
        "        # dydx = 1 if x > 0 else 0\n",
        "        # so dLdx = dLdy if x > 0 else 0\n",
        "        # so we can just multiply dLdy with a mask of x > 0\n",
        "        mask = self.x > 0\n",
        "        return dLdy * mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbVYUKLVn57g"
      },
      "source": [
        "### 2. Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKz7yNiQfXOP"
      },
      "outputs": [],
      "source": [
        "class Sigmoid(Activation):\n",
        "    name = 'sigmoid'\n",
        "\n",
        "    def calc(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def backward(self, dLdy, lr):\n",
        "        # dLdy is dLdx from next layer\n",
        "        # dLdx = dLdy * dydx (i.e. dLdinput)\n",
        "        # dydx = y * (1 - y)\n",
        "        # so dLdx = dLdy * y * (1 - y)\n",
        "        # so we can just multiply dLdy with y * (1 - y)\n",
        "        y = self.calc(self.x)\n",
        "        return dLdy * y * (1 - y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZOiGFWrJ4U8"
      },
      "source": [
        "### 3. Tanh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygAP-h45J4U8"
      },
      "outputs": [],
      "source": [
        "class TanH(Activation):\n",
        "    name = 'tanh'\n",
        "\n",
        "    def calc(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def backward(self, dLdy, lr):\n",
        "        # dLdy is dLdx from next layer\n",
        "        # dLdx = dLdy * dydx (i.e. dLdinput)\n",
        "        # dydx = 1 - y^2\n",
        "        # so dLdx = dLdy * (1 - y^2)\n",
        "        # so we can just multiply dLdy with (1 - y^2)\n",
        "        y = self.calc(self.x)\n",
        "        return dLdy * (1 - y**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5VfLcYFJ4U8"
      },
      "source": [
        "### Deserialization Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDu8__R8J4U9"
      },
      "outputs": [],
      "source": [
        "def get_activation(activation, allow_none=True):\n",
        "    activations = [ReLU, Sigmoid, TanH]\n",
        "    if isinstance(activation, str):\n",
        "        acts = {\n",
        "            '': None,\n",
        "            **{act.name: act for act in activations}\n",
        "        }\n",
        "        assert activation in acts.keys(), f\"activation must be either {','.join(acts.keys())}\"\n",
        "        activation = acts[activation]\n",
        "        if activation is not None:\n",
        "            activation = activation()\n",
        "    if not allow_none and activation is None:\n",
        "        raise TypeError(\"Activation must not be None if allow_none = true\")\n",
        "    if activation is not None and not isinstance(activation, Activation):\n",
        "        raise TypeError(\"Activation must be inherited from Activation base class\")\n",
        "    return activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTC3zAoDnBgz"
      },
      "source": [
        "## B. Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1k3MFPwrhOY"
      },
      "outputs": [],
      "source": [
        "# Calculation without loops and without numpy sliding window tricks\n",
        "def vector_calc(x, out_size, kernel_size, stride):\n",
        "    bsz, x_c, x_h, x_w = x.shape\n",
        "    y_h, y_w = out_size\n",
        "    k_h, k_w = kernel_size\n",
        "    # setup for advanced indexing\n",
        "    i0 = np.repeat(np.arange(k_h), k_h) # indexing of rows from kernel\n",
        "    i1 = np.repeat(np.arange(0, x_h-k_h+1, stride), y_h) # indexing of rows from input\n",
        "    j0 = np.tile(np.arange(k_w), k_w) # indexing of columns from kernel\n",
        "    j1 = np.tile(np.arange(0, x_w-k_h+1, stride), y_w) # indexing of columns from input\n",
        "    # construct full index matrix by adding both indices i.e. shifting by kernel\n",
        "    i = i0.reshape(-1,1)+i1.reshape(1,-1)\n",
        "    j = j0.reshape(-1,1)+j1.reshape(1,-1)\n",
        "    # broadcast over all channels\n",
        "    i = np.tile(i, (x_c, 1, 1))\n",
        "    j = np.tile(j, (x_c, 1, 1))\n",
        "    # broadcast over all images in batch\n",
        "    i = np.tile(i, (bsz, 1, 1, 1))\n",
        "    j = np.tile(j, (bsz, 1, 1, 1))\n",
        "    i_h, i_w = i.shape[2], i.shape[3]\n",
        "    # same shape for channel indices\n",
        "    k = np.arange(x_c).reshape(1,-1,1,1)\n",
        "    k = np.repeat(k, i_w, axis=3)\n",
        "    k = np.repeat(k, i_h, axis=2)\n",
        "    k = np.repeat(k, bsz, axis=0)\n",
        "    # same shape for batch indices\n",
        "    b = np.arange(bsz).reshape(-1,1,1,1)\n",
        "    b = np.repeat(b, i_w, axis=3)\n",
        "    b = np.repeat(b, i_h, axis=2)\n",
        "    b = np.repeat(b, x_c, axis=1)\n",
        "    # index from input\n",
        "    # (batch_size, in_channels, kernel_height * kernel_width, out_height * out_width)\n",
        "    select_img = x[b, k, i, j]\n",
        "    return select_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbyaf7H_nJH2"
      },
      "source": [
        "### 1. Convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT7Fl7ejfXON"
      },
      "outputs": [],
      "source": [
        "class Convolution(Layer):\n",
        "    name = 'conv2d'\n",
        "    DEFAULT_SIZE = (1,3,16,16)\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_size=DEFAULT_SIZE,\n",
        "        padding=0,\n",
        "        n_kernels=3,\n",
        "        kernel_size=(3,3),\n",
        "        activation: Activation|str|None='relu',\n",
        "        stride=1\n",
        "    ):\n",
        "        self.in_size = in_size # (batch_size, n_channels, height, width)\n",
        "        self.padding = padding # padding size on each side\n",
        "        self.n_kernels = n_kernels # number of kernels *per input channel*\n",
        "        self.kernel_size = kernel_size # (height, width)\n",
        "        self.stride = stride # stride size to shift kernel\n",
        "        self.activation = get_activation(activation)\n",
        "\n",
        "    def get_out_size(self, in_size):\n",
        "        return (\n",
        "            in_size[0],\n",
        "            self.n_kernels,\n",
        "            ((in_size[2] - self.kernel_size[0] + 2 * self.padding) // self.stride) + 1,\n",
        "            ((in_size[3] - self.kernel_size[1] + 2 * self.padding) // self.stride) + 1,\n",
        "        )\n",
        "\n",
        "    def compile(self):\n",
        "        # generate output size\n",
        "        # (batch_size, n_kernels, out_height, out_width) so out_channels = n_kernels\n",
        "        self.out_size = self.get_out_size(self.in_size)\n",
        "        # initialize kernels with random values and normalize them\n",
        "        # (in_channels, n_kernels, kernel_height, kernel_width)\n",
        "        self.kernels = np.random.randn(\n",
        "            self.in_size[1],\n",
        "            self.n_kernels,\n",
        "            *self.kernel_size\n",
        "        )\n",
        "        self.kernels /= self.kernel_size[0] * self.kernel_size[1]\n",
        "        # (n_kernels, out_height, out_width)\n",
        "        self.bias = np.zeros((self.n_kernels, *self.out_size[-2:]))\n",
        "\n",
        "    def vectorize_regions(self, x):\n",
        "        _, _, y_h, y_w = self.out_size\n",
        "        return vector_calc(x, (y_h, y_w), self.kernel_size, self.stride)\n",
        "\n",
        "    def vectorize_backregions(self, x):\n",
        "        bs, c, x_h, x_w = x.shape\n",
        "        _, _, in_h, in_w = self.in_size\n",
        "        # to accomodate stride, we need to pad x with zeros after, before, and between each element\n",
        "        s = self.stride\n",
        "        shape = (bs, c, (x_h+1)*(s-1) + x_h, (x_w+1)*(s-1) + x_w)\n",
        "        padded = np.zeros(shape)\n",
        "        padded[:, :, s-1::s, s-1::s] = x\n",
        "        # calculate padding size\n",
        "        y_pad_h = in_h - padded.shape[2]\n",
        "        y_pad_h = y_pad_h + ((self.kernel_size[0] // 2) * 2)\n",
        "        y_pad_h = y_pad_h // 2\n",
        "        y_pad_w = in_w - padded.shape[3]\n",
        "        y_pad_w = y_pad_w + ((self.kernel_size[1] // 2) * 2)\n",
        "        y_pad_w = y_pad_w // 2\n",
        "        # pad dLdy\n",
        "        padded = np.pad(padded, ((0,0), (0,0), (y_pad_h, y_pad_h), (y_pad_w, y_pad_w)), 'constant')\n",
        "        # vectorize receptive fields\n",
        "        regions = vector_calc(padded, (in_h, in_w), self.kernel_size, 1)\n",
        "        return regions\n",
        "\n",
        "    def forward(self, x):\n",
        "        self._update_batch(x.shape[0])\n",
        "        # pad x with zeros\n",
        "        x = np.pad(x, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)), 'constant')\n",
        "        # cache x for backpropagation\n",
        "        self.x = x\n",
        "        # vectorize receptive fields\n",
        "        regions = self.vectorize_regions(x) # (batch_size, in_channels, kernel_height * kernel_width, out_height * out_width)\n",
        "        # reshape kernels to (in_channels, n_kernels, kernel_height * kernel_width)\n",
        "        kernels = self.kernels.reshape(self.in_size[1], self.n_kernels, -1) # (in_channels, n_kernels, kernel_height * kernel_width)\n",
        "        # now we can just do matrix multiplication\n",
        "        out = kernels @ regions # (batch_size, in_channels, n_kernels, out_height * out_width)\n",
        "        # add together among channels\n",
        "        out = out.sum(axis=1) # (batch_size, n_kernels, out_height * out_width)\n",
        "        # add bias\n",
        "        out += self.bias.reshape(self.n_kernels, -1) # (batch_size, n_kernels, out_height * out_width) (bias is broadcasted)\n",
        "        # reshape to output shape\n",
        "        out = out.reshape(self.out_size) # (batch_size, n_kernels, out_height, out_width)\n",
        "        # apply activation\n",
        "        if self.activation is not None:\n",
        "            out = self.activation.forward(out)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dLdy, lr):\n",
        "        if self.activation is not None:\n",
        "            dLdy = self.activation.backward(dLdy, lr)\n",
        "        # dLdy shape: (batch_size, n_kernels, out_height, out_width)\n",
        "        # first we calculate dLdk and dLdb\n",
        "        # we do that with the convolution dLdk = x * dLdy and dLdb = 1 * dLdy where * is convolution\n",
        "        # this is a property of the convolution operation when backpropagating\n",
        "        # need to dilate dLdy according to stride with zeros\n",
        "        dilate_h = dLdy.shape[2] * self.stride - (self.stride - 1) if self.stride > 1 else dLdy.shape[2]\n",
        "        dilate_w = dLdy.shape[3] * self.stride - (self.stride - 1) if self.stride > 1 else dLdy.shape[3]\n",
        "        dLdy_dilate = np.zeros((dLdy.shape[0], dLdy.shape[1], dilate_h, dilate_w))\n",
        "        dLdy_dilate[:, :, ::self.stride, ::self.stride] = dLdy\n",
        "        # prepare dLdy for convolution\n",
        "        conv_dLdy = dLdy_dilate.reshape(dLdy_dilate.shape[0], 1, dLdy_dilate.shape[1], -1) # (batch_size, 1, n_kernels, out_height * out_width)\n",
        "        # we can use the same trick as in forward pass to vectorize receptive fields\n",
        "        x_regions = vector_calc(self.x, self.kernel_size, (dLdy_dilate.shape[2], dLdy_dilate.shape[3]), 1)\n",
        "        # calculate dLdk and dLdb\n",
        "        dLdk = conv_dLdy @ x_regions # (batch_size, in_channels, kernel_height * kernel_width, n_kernels)\n",
        "        dLdk = dLdk.transpose(0,1,3,2) # (batch_size, in_channels, n_kernels, kernel_height * kernel_width)\n",
        "        dLdk = dLdk.sum(axis=0) # (in_channels, n_kernels, kernel_height * kernel_width) (sum over batch and receptive fields)\n",
        "        dLdb = dLdy.sum(axis=0) # (n_kernels, out_height, out_width) (sum over batch and receptive field)\n",
        "        # update weights\n",
        "        self.kernels -= lr * dLdk.reshape(self.kernels.shape) # (in_channels, n_kernels, kernel_height, kernel_width)\n",
        "        self.bias -= lr * dLdb # (n_kernels, out_height, out_width)\n",
        "        # now we calculate dLdx\n",
        "        # this one is a bit funky, we need to calculate dLdx = dLdy * k\n",
        "        # and we need to pad dLdy with zeros until the output will be the same size as x\n",
        "        # why? because we need to align the contributions of each pixel in x to the output\n",
        "        # so we need to calculate the padding size which must result in whatever x.shape + 1 is\n",
        "        # vectorize receptive fields\n",
        "        regions = self.vectorize_backregions(dLdy)\n",
        "        # reshape kernels to (in_channels, n_kernels, kernel_height * kernel_width)\n",
        "        kernels = self.kernels.reshape(self.in_size[1], self.n_kernels, -1)\n",
        "        # transpose kernels to (n_kernels, in_channels, kernel_height * kernel_width)\n",
        "        kernels = kernels.transpose(1,0,2)\n",
        "        # now we can just do matrix multiplication\n",
        "        dLdx = kernels @ regions # (batch_size, n_kernels, in_channels, in_height * in_width)\n",
        "        # add together among kernels\n",
        "        dLdx = dLdx.sum(axis=1)\n",
        "        # reshape to input shape\n",
        "        dLdx = dLdx.reshape(self.in_size)\n",
        "        # return dLdx\n",
        "        return dLdx\n",
        "\n",
        "    @classmethod\n",
        "    def _deserialize(cls, data):\n",
        "        p = data['params']\n",
        "        # we can auto infer in_size, n_kernels, and kernel_size from kernel shape\n",
        "        # (no_channel, kernel_size[0], kernel_size[1], n_kernels)\n",
        "        k = np.array(p['kernel'])\n",
        "        no_channel, ks0, ks1, n_kernels = k.shape\n",
        "        kernel_size = (ks0, ks1)\n",
        "        if 'img_size' in p:\n",
        "            def_size = tuple(p['img_size'])\n",
        "        else:\n",
        "            def_size = (cls.DEFAULT_SIZE[-2], cls.DEFAULT_SIZE[-1])\n",
        "        c = cls(\n",
        "            in_size=(None, no_channel, *def_size),\n",
        "            kernel_size=kernel_size,\n",
        "            n_kernels=n_kernels,\n",
        "            **data['meta']\n",
        "        )\n",
        "        # set kernels\n",
        "        c.compile()\n",
        "        k = k.transpose(0, 3, 1, 2) # (no_channel, n_kernels, kernel_size[0], kernel_size[1])\n",
        "        assert k.shape == c.kernels.shape, f\"kernel shape mismatch: {k.shape} != {c.kernels.shape}\"\n",
        "        c.kernels = k\n",
        "        # set bias\n",
        "        b = np.array(p['bias'])\n",
        "        if len(b.shape) == 1:  # parameter sharing, duplicate it\n",
        "            b = b.repeat(c.out_size[-2] * c.out_size[-1]).reshape(c.bias.shape)\n",
        "        assert b.shape == c.bias.shape, f\"bias shape mismatch: {b.shape} != {c.bias.shape}\"\n",
        "        c.bias = b\n",
        "\n",
        "        return c\n",
        "\n",
        "\n",
        "    def _serialize(self):\n",
        "        return [{\n",
        "            'type': 'conv2d',\n",
        "            'params': {\n",
        "                # restore to (no_channel, kernel_size[0], kernel_size[1], n_kernels)\n",
        "                # for compatibility\n",
        "                'kernel': self.kernels.transpose(0, 2, 3, 1).tolist(),\n",
        "                'bias': self.bias.tolist(),\n",
        "                'img_size': self.in_size[-2:],\n",
        "            },\n",
        "            'meta': {\n",
        "                'padding': self.padding,\n",
        "                'stride': self.stride,\n",
        "                'activation': None if self.activation is None else self.activation.name,\n",
        "            }\n",
        "        }]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQjn8udrJ4U9",
        "outputId": "593c4853-2c0a-48cf-94a7-ae8cda19b151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x (1, 1, 32, 32)\n",
            "[[[[ 0.95172323 -1.48659433 -0.01935405 ... -0.38011853  1.85622064\n",
            "     1.03372016]\n",
            "   [ 0.16750962 -0.53118492  0.14296131 ...  1.75401119 -0.69414117\n",
            "     0.37768782]\n",
            "   [-0.490602    0.11356264  0.00663914 ...  0.28692275  1.42143583\n",
            "     1.94405463]\n",
            "   ...\n",
            "   [-0.03343196  0.56678553  0.67181207 ... -1.09701458 -0.829446\n",
            "    -0.85642891]\n",
            "   [-1.02649227 -0.56310046  0.2073629  ...  0.36762804  0.65842512\n",
            "     1.56222553]\n",
            "   [ 0.07713261 -0.3688733  -1.39069608 ...  2.29601218 -1.170719\n",
            "     0.54954297]]]]\n",
            "out (1, 1, 8, 8)\n",
            "[[[[0.19997201 0.24508563 0.14150687 0.41986734 0.         0.06733596\n",
            "    0.         0.04619712]\n",
            "   [0.29393066 0.20844106 0.09891817 0.         0.12955816 0.0998903\n",
            "    0.         0.        ]\n",
            "   [0.12011249 0.         0.30744752 0.         0.         0.\n",
            "    0.16278822 0.        ]\n",
            "   [0.         0.         0.         0.         0.31833272 0.28292788\n",
            "    0.         0.        ]\n",
            "   [0.45731917 0.13722932 0.34579298 0.10483424 0.         0.\n",
            "    0.10049178 0.09106039]\n",
            "   [0.         0.08920086 0.26303513 0.19181146 0.         0.\n",
            "    0.         0.07568032]\n",
            "   [0.28536381 0.         0.16253594 0.         0.29317332 0.11585396\n",
            "    0.39423649 0.        ]\n",
            "   [0.0630815  0.         0.11545468 0.         0.22638687 0.\n",
            "    0.         0.06129911]]]]\n",
            "dLdy (1, 1, 8, 8)\n",
            "[[[[2 3 5 4 4 5 4 2]\n",
            "   [8 2 8 6 3 2 1 1]\n",
            "   [7 5 4 1 7 2 4 4]\n",
            "   [8 8 6 3 8 4 3 6]\n",
            "   [3 3 2 1 6 6 3 8]\n",
            "   [8 8 5 4 6 8 3 2]\n",
            "   [5 6 5 5 5 6 2 6]\n",
            "   [4 2 5 4 3 3 7 1]]]]\n",
            "dLdx (1, 1, 32, 32)\n",
            "[[[[ -4.37853659  -2.03877998  12.84859661 ...  -2.03877998\n",
            "     12.84859661   2.31963898]\n",
            "   [-11.39012771   8.0211163    8.67394632 ...   8.0211163\n",
            "      8.67394632  -7.23491752]\n",
            "   [-12.25548701  -3.90664541  10.84593396 ...  -3.90664541\n",
            "     10.84593396  -4.38031173]\n",
            "   ...\n",
            "   [-22.78025542  16.04223261  17.34789264 ...   4.01055815\n",
            "      4.33697316  -3.61745876]\n",
            "   [-24.51097402  -7.81329082  21.69186792 ...  -1.95332271\n",
            "      5.42296698  -2.19015586]\n",
            "   [-12.32094765  -1.59716199   3.77237002 ...  -0.3992905\n",
            "      0.94309251  -6.45951334]]]]\n"
          ]
        }
      ],
      "source": [
        "# test conv backprop\n",
        "x = np.random.randn(1,1,32,32)\n",
        "print('x', x.shape)\n",
        "print(x)\n",
        "conv = Convolution(in_size=x.shape, padding=0, n_kernels=1, kernel_size=(4,4), stride=4)\n",
        "conv.compile()\n",
        "out = conv.forward(x)\n",
        "print('out', out.shape)\n",
        "print(out)\n",
        "dLdy = np.random.randint(1, 9, out.shape)\n",
        "print('dLdy', dLdy.shape)\n",
        "print(dLdy)\n",
        "dLdx = conv.backward(dLdy, 0.1)\n",
        "print('dLdx', dLdx.shape)\n",
        "print(dLdx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Soy4RxWnnPqf"
      },
      "source": [
        "### 2. Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUO7L9YdfXOO"
      },
      "outputs": [],
      "source": [
        "class Pooling(Layer):\n",
        "    name = '_pooling2d'\n",
        "    def __init__(self, pool_size=(2,2), stride=2, mode='max'):\n",
        "        assert mode in ['max', 'avg'], \"mode must be either 'max' or 'avg'\"\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "        self.mode = mode\n",
        "        self.x = None\n",
        "        self.mask = None\n",
        "        self.max_ids = None\n",
        "\n",
        "    def vectorize_pools(self, x):\n",
        "        _, _, x_h, x_w = x.shape\n",
        "        y_h = (x_h - self.pool_size[0]) // self.stride + 1\n",
        "        y_w = (x_w - self.pool_size[1]) // self.stride + 1\n",
        "        return vector_calc(x, (y_h, y_w), self.pool_size, self.stride)\n",
        "\n",
        "    def vectorize_backpools(self, x):\n",
        "        bs, c, x_h, x_w = x.shape\n",
        "        _, _, in_h, in_w = self.x.shape\n",
        "        # to accomodate stride, we need to pad x with zeros after, before, and between each element\n",
        "        s = self.stride\n",
        "        shape = (bs, c, (x_h+1)*(s-1) + x_h, (x_w+1)*(s-1) + x_w)\n",
        "        padded = np.zeros(shape)\n",
        "        padded[:, :, s-1::s, s-1::s] = x\n",
        "        # calculate padding size\n",
        "        y_pad_h = in_h - padded.shape[2]\n",
        "        y_pad_h = y_pad_h + ((self.pool_size[0] // 2) * 2)\n",
        "        y_pad_h = y_pad_h // 2\n",
        "        y_pad_w = in_w - padded.shape[3]\n",
        "        y_pad_w = y_pad_w + ((self.pool_size[1] // 2) * 2)\n",
        "        y_pad_w = y_pad_w // 2\n",
        "        # pad dLdy\n",
        "        padded = np.pad(padded, ((0,0), (0,0), (y_pad_h, y_pad_h), (y_pad_w, y_pad_w)), 'constant')\n",
        "        # vectorize receptive fields\n",
        "        regions = vector_calc(padded, (in_h, in_w), self.pool_size, 1)\n",
        "        return regions\n",
        "\n",
        "    def get_out_size(self, in_size):\n",
        "        # adjust input size to be divisible by pool size\n",
        "        in_size = list(in_size)\n",
        "        in_size[2] = in_size[2] - (in_size[2] % self.pool_size[0])\n",
        "        in_size[3] = in_size[3] - (in_size[3] % self.pool_size[1])\n",
        "        return (\n",
        "            in_size[0],\n",
        "            in_size[1],\n",
        "            (in_size[2] - self.pool_size[0]) // self.stride + 1,\n",
        "            (in_size[3] - self.pool_size[1]) // self.stride + 1,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # crop last rows and columns to make it divisible by pool size\n",
        "        self.is_odd = x.shape[2] % self.pool_size[0] != 0 or x.shape[3] % self.pool_size[1] != 0\n",
        "        x = x[:, :, :x.shape[2] - (x.shape[2] % self.pool_size[0]), :x.shape[3] - (x.shape[3] % self.pool_size[1])]\n",
        "        self.x = x\n",
        "        self._update_batch(x.shape[0])\n",
        "        out_size = self.get_out_size(x.shape)\n",
        "        # vectorize pools so that the regions are columns\n",
        "        pools = self.vectorize_pools(x)\n",
        "        # get max or average\n",
        "        if self.mode == 'max':\n",
        "            self.max_ids = np.argmax(pools, axis=2)\n",
        "            out = np.max(pools, axis=2)\n",
        "        elif self.mode == 'avg':\n",
        "            out = np.mean(pools, axis=2)\n",
        "        # reshape to output shape\n",
        "        out = out.reshape(out_size)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dLdy, lr):\n",
        "        # dLdy shape: (batch_size, n_channels, out_height, out_width)\n",
        "        # no weights to update, just need to calculate dLdx\n",
        "        # now for mean pooling, we just need to make a kernel of 1/pool_size and convolve it with dLdy\n",
        "        if self.mode == 'avg':\n",
        "            # same as convolution, we need to pad dLdy with zeros until the output will be the same size as x\n",
        "            # vectorize pools\n",
        "            pools = self.vectorize_backpools(dLdy) # (batch_size, n_channels, pool_height * pool_width, out_height * out_width)\n",
        "            k = np.ones(self.pool_size) / (self.pool_size[0] * self.pool_size[1])\n",
        "            # reshape kernel to (1, 1, kernel_height * kernel_width)\n",
        "            k = k.reshape(1, 1, k.shape[0] * k.shape[1])\n",
        "            # convolve\n",
        "            dLdx = k @ pools\n",
        "            dLdx = dLdx.reshape(self.x.shape)\n",
        "        # for max pooling, we need to find the max value in each pool and set it to 1, the rest to 0\n",
        "        # and then we multiply elementwise with dLdy\n",
        "        elif self.mode == 'max':\n",
        "            # initialize the gradient tensor with zeros to the same shape as the input x\n",
        "            dLdx = np.zeros_like(self.x)\n",
        "            # determine the positions in the original input x where each dLdy value will be placed\n",
        "            batch_idx, channel_idx, out_i, out_j = np.indices(dLdy.shape)\n",
        "            # reshape max_ids back to 2 dimensions\n",
        "            self.max_ids = self.max_ids.reshape(self.max_ids.shape[0], self.max_ids.shape[1], dLdy.shape[2], dLdy.shape[3])\n",
        "            # get row and column indices of the max values from self.max_ids\n",
        "            max_pos_i, max_pos_j = divmod(self.max_ids, self.pool_size[1])\n",
        "            # calculate the positions in the original input x\n",
        "            input_i = out_i * self.stride + max_pos_i[batch_idx, channel_idx, out_i, out_j]\n",
        "            input_j = out_j * self.stride + max_pos_j[batch_idx, channel_idx, out_i, out_j]\n",
        "            # use advanced indexing to update dLdx\n",
        "            dLdx[batch_idx, channel_idx, input_i, input_j] = dLdy[batch_idx, channel_idx, out_i, out_j]\n",
        "        # add zeros back to the end if the input was odd\n",
        "        if self.is_odd:\n",
        "            dLdx = np.pad(dLdx, ((0,0), (0,0), (0,1), (0,1)), 'constant')\n",
        "        # return dLdx\n",
        "        return dLdx\n",
        "\n",
        "    @classmethod\n",
        "    def _deserialize(cls, data):\n",
        "        mode = data['type'].split('_')[0]\n",
        "        return cls(\n",
        "            mode=mode,\n",
        "            **data['meta'],\n",
        "        )\n",
        "\n",
        "    def _serialize(self):\n",
        "        return [{\n",
        "            'type': f'{self.mode}{self.name}',\n",
        "            'meta': {\n",
        "                'pool_size': self.pool_size,\n",
        "                'stride': self.stride,\n",
        "            }\n",
        "        }]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP57ebehJ4U-",
        "outputId": "f4819b89-735e-4f1f-9109-0a33a35e5299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1, 12, 12)\n",
            "[[[[-0.37613749  1.7419758   0.51781062  0.49499937 -0.48927446\n",
            "    -0.16975034  0.77617644 -2.06831327  0.37353925  0.85677623\n",
            "    -0.17441137 -0.69373026]\n",
            "   [ 0.13131136  0.59365921  0.66604824 -0.69068809 -1.21705825\n",
            "     0.83892934 -0.97689771 -0.77746656  2.75014632  1.26737113\n",
            "     0.0460793   0.93298   ]\n",
            "   [-0.04665596 -1.41358926 -1.92750485 -0.01181503 -0.54554272\n",
            "    -1.13387094  0.75944302 -0.60654733 -0.57594301 -0.34294087\n",
            "     0.27676555 -0.2841054 ]\n",
            "   [ 1.15016268  0.37874626  0.21857591 -0.10558039 -0.39612805\n",
            "    -0.76272858 -0.55492714 -1.75383943  0.06215821  0.09305066\n",
            "    -0.21747143 -0.61696121]\n",
            "   [ 0.80142675 -1.14099451  0.12197229  0.7667177   1.32087495\n",
            "     0.55070667  0.77427229  0.50330044 -0.39579064 -1.0141134\n",
            "    -0.58847999  0.43566313]\n",
            "   [ 0.54821643 -1.74123915 -1.08298406  0.49194784  0.77706953\n",
            "     0.33717693  1.28960878  1.72386019  0.51835427 -0.75969429\n",
            "    -0.26438119  0.42247968]\n",
            "   [-0.68997485 -1.04850389  0.71048726  0.98961062 -0.61208364\n",
            "    -0.89625412 -1.27563229  0.01240374  0.32568234  1.32161386\n",
            "    -0.65077355  0.92648031]\n",
            "   [-0.75738216 -0.87321292  0.74684007  1.70844674  0.95643224\n",
            "    -1.11052712  1.08315444  0.27319914 -2.40185192  1.14225272\n",
            "     1.61008763 -0.56831596]\n",
            "   [ 0.69801425  0.35153997 -1.08008724  1.31797037  0.13772766\n",
            "    -1.4248108  -0.50717233 -0.38170124  0.07685013 -1.84960928\n",
            "     0.29443106  0.4381418 ]\n",
            "   [ 0.98433219  0.06605911  1.10079877 -0.65713165 -2.48834847\n",
            "     1.17623351  0.7021139   0.0540826  -0.71374557  0.39934036\n",
            "    -0.63088551 -0.34690184]\n",
            "   [-0.49670577  1.39892914 -1.31978159  0.15496332 -0.75295565\n",
            "    -1.44589229 -0.55187849 -0.65961801  1.24216543 -0.931115\n",
            "    -1.35107479 -0.11922741]\n",
            "   [ 1.62663337 -1.17072282  0.25599921 -0.87839772  0.43910559\n",
            "    -0.6864648   0.9705442  -1.81404114  0.3944777   0.67333602\n",
            "    -0.99229066 -0.54677023]]]]\n",
            "(1, 1, 6, 6)\n",
            "[[[[ 0.52270222  0.24704254 -0.25928843 -0.76162527  1.31195823\n",
            "     0.02772942]\n",
            "   [ 0.01716593 -0.45658109 -0.70956757 -0.53896772 -0.19091875\n",
            "    -0.21044312]\n",
            "   [-0.38314762  0.07441344  0.74645702  1.07276043 -0.41281101\n",
            "     0.00132041]\n",
            "   [-0.84226845  1.03884617 -0.41560816  0.02328126  0.09692425\n",
            "     0.3293696 ]\n",
            "   [ 0.52498638  0.17038756 -0.64979952 -0.03316927 -0.52179109\n",
            "    -0.06130362]\n",
            "   [ 0.33953348 -0.4468042  -0.61155179 -0.51374836  0.34471604\n",
            "    -0.75234077]]]]\n",
            "(1, 1, 6, 6)\n",
            "[[[[1 8 4 5 6 1]\n",
            "   [3 3 6 7 7 7]\n",
            "   [1 3 7 1 5 1]\n",
            "   [2 4 8 8 5 5]\n",
            "   [7 2 2 7 3 1]\n",
            "   [7 6 7 8 7 1]]]]\n",
            "(1, 1, 12, 12)\n",
            "[[[[0.25 0.25 2.   2.   1.   1.   1.25 1.25 1.5  1.5  0.25 0.25]\n",
            "   [0.25 0.25 2.   2.   1.   1.   1.25 1.25 1.5  1.5  0.25 0.25]\n",
            "   [0.75 0.75 0.75 0.75 1.5  1.5  1.75 1.75 1.75 1.75 1.75 1.75]\n",
            "   [0.75 0.75 0.75 0.75 1.5  1.5  1.75 1.75 1.75 1.75 1.75 1.75]\n",
            "   [0.25 0.25 0.75 0.75 1.75 1.75 0.25 0.25 1.25 1.25 0.25 0.25]\n",
            "   [0.25 0.25 0.75 0.75 1.75 1.75 0.25 0.25 1.25 1.25 0.25 0.25]\n",
            "   [0.5  0.5  1.   1.   2.   2.   2.   2.   1.25 1.25 1.25 1.25]\n",
            "   [0.5  0.5  1.   1.   2.   2.   2.   2.   1.25 1.25 1.25 1.25]\n",
            "   [1.75 1.75 0.5  0.5  0.5  0.5  1.75 1.75 0.75 0.75 0.25 0.25]\n",
            "   [1.75 1.75 0.5  0.5  0.5  0.5  1.75 1.75 0.75 0.75 0.25 0.25]\n",
            "   [1.75 1.75 1.5  1.5  1.75 1.75 2.   2.   1.75 1.75 0.25 0.25]\n",
            "   [1.75 1.75 1.5  1.5  1.75 1.75 2.   2.   1.75 1.75 0.25 0.25]]]]\n"
          ]
        }
      ],
      "source": [
        "# test pooling backprop\n",
        "x = np.random.randn(1,1,12,12)\n",
        "print(x.shape)\n",
        "print(x)\n",
        "pool = Pooling(pool_size=(2,2), stride=2, mode='avg')\n",
        "out = pool.forward(x)\n",
        "print(out.shape)\n",
        "print(out)\n",
        "dLdy = np.random.randint(1, 9, out.shape)\n",
        "print(dLdy.shape)\n",
        "print(dLdy)\n",
        "dLdx = pool.backward(dLdy, 0.1)\n",
        "print(dLdx.shape)\n",
        "print(dLdx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhWldbdznuNs"
      },
      "source": [
        "### 3. Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Xl2WEzwfXOP"
      },
      "outputs": [],
      "source": [
        "class Flatten(Layer):\n",
        "    name = 'flatten'\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.in_size = 0\n",
        "\n",
        "    def get_out_size(self, in_size):\n",
        "        s = 1\n",
        "        for i in range(1, len(in_size)):\n",
        "            s *= in_size[i]\n",
        "        return (None, s)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return x.reshape(x.shape[0], -1)\n",
        "\n",
        "    def backward(self, dLdy, lr):\n",
        "        # TODO: test this\n",
        "        return dLdy.reshape(self.x.shape)\n",
        "\n",
        "    @classmethod\n",
        "    def _deserialize(cls, _):\n",
        "        return cls()\n",
        "\n",
        "    def _serialize(self):\n",
        "        return [{\n",
        "            'type': 'flatten',\n",
        "        }]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB8VOmAYnwvp"
      },
      "source": [
        "### 4. Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwU4P-GefXOP"
      },
      "outputs": [],
      "source": [
        "class Linear(Layer):\n",
        "    name = 'linear'\n",
        "    def __init__(self, in_size=10, out_size=10):\n",
        "        self.in_size = (None, in_size)\n",
        "        self.out_size = (None, out_size)\n",
        "        self.x = None\n",
        "\n",
        "    def get_out_size(self, in_size):\n",
        "        return self.out_size\n",
        "\n",
        "    def compile(self):\n",
        "        # initialize weights with random values and normalize them\n",
        "        self.weights = np.random.randn(\n",
        "            self.in_size[-1],\n",
        "            self.out_size[-1],\n",
        "        ) # (in_features, out_features)\n",
        "        self.weights /= self.in_size[-1]\n",
        "        self.bias = np.zeros(self.out_size[-1]) # (out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        assert x.shape[-1] == self.weights.shape[0], f\"input shape mismatch: {x.shape[-1]} != {self.weights.shape[0]}\"\n",
        "        out = x @ self.weights # (batch_size, out_features)\n",
        "        out += self.bias # (batch_size, out_features)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dLdy, lr):\n",
        "        dLdw = self.x.T @ dLdy # (in_features, out_features)\n",
        "        dLdb = dLdy.sum(axis=0) # (out_features)\n",
        "        # update weights and bias\n",
        "        self.weights -= lr * dLdw\n",
        "        self.bias -= lr * dLdb\n",
        "        # calculate dLdx\n",
        "        dLdx = dLdy @ self.weights.T # (batch_size, in_features)\n",
        "        return dLdx\n",
        "\n",
        "    def _load_weights(self, k, b):\n",
        "        self.compile()\n",
        "        # set weight\n",
        "        assert k.shape == self.weights.shape, f\"kernel shape mismatch: {k.shape} != {self.weights.shape}\"\n",
        "        self.weights = k\n",
        "        # set bias\n",
        "        assert b.shape == self.bias.shape, f\"bias shape mismatch: {b.shape} != {self.bias.shape}\"\n",
        "        self.bias = b\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def _deserialize(cls, data):\n",
        "        p = data['params']\n",
        "        k = np.array(p['kernel'])\n",
        "        in_size, out_size = k.shape\n",
        "        c = cls(\n",
        "            in_size=in_size,\n",
        "            out_size=out_size,\n",
        "            **data['meta'],\n",
        "        )\n",
        "        # set weight\n",
        "        c._load_weights(k, np.array(p['bias']))\n",
        "        return c\n",
        "\n",
        "    def _serialize(self):\n",
        "        return [{\n",
        "            'type': 'linear',\n",
        "            'params': {\n",
        "                'kernel': self.weights.tolist(),\n",
        "                'bias': self.bias.tolist(),\n",
        "            }\n",
        "        }]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oggbGV6ZnyeJ"
      },
      "source": [
        "### 5. Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiagWKdufXOP"
      },
      "outputs": [],
      "source": [
        "class Dense(Linear):\n",
        "    name = 'dense'\n",
        "    def __init__(self, inputs=0, units=10, activation:Activation|str='sigmoid'):\n",
        "        super().__init__(inputs, units)\n",
        "        self.activation = get_activation(activation, allow_none=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = super().forward(x)\n",
        "        out = self.activation.forward(out)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dLdy, lr):\n",
        "        dLdx = self.activation.backward(dLdy, lr)\n",
        "        dLdx = super().backward(dLdx, lr)\n",
        "        return dLdx\n",
        "\n",
        "    @classmethod\n",
        "    def _deserialize(cls, data):\n",
        "        p = data['params']\n",
        "        k = np.array(p['kernel'])\n",
        "        in_size, out_size = k.shape\n",
        "        c = cls(\n",
        "            inputs=in_size,\n",
        "            units=out_size,\n",
        "            **data['meta']\n",
        "        )\n",
        "        # set weight\n",
        "        c._load_weights(k, np.array(p['bias']))\n",
        "        return c\n",
        "\n",
        "    def _serialize(self):\n",
        "        return [{\n",
        "            'type': 'dense',\n",
        "            'params': {\n",
        "                'kernel': self.weights.tolist(),\n",
        "                'bias': self.bias.tolist(),\n",
        "            },\n",
        "            'meta': {\n",
        "                'activation': self.activation.name,\n",
        "            }\n",
        "        }]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfcRL1WXJ4VA"
      },
      "source": [
        "### 6. LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-yTLou6J4VA"
      },
      "outputs": [],
      "source": [
        "class LSTM(Layer):\n",
        "    name = 'lstm'\n",
        "\n",
        "    def __init__(self, in_size=5, units=10, return_sequences=False):\n",
        "        self.in_size = (None, None, in_size)\n",
        "        self.out_size = (None, None if return_sequences else 1, units)\n",
        "        self.x = None\n",
        "        self.sigmoid = Sigmoid()\n",
        "        self.tanh = TanH()\n",
        "        self.return_sequences = return_sequences\n",
        "\n",
        "    def get_out_size(self, in_size):\n",
        "        return self.out_size\n",
        "\n",
        "    def compile(self):\n",
        "        self.out_size = (None, None if self.return_sequences else 1, self.out_size[-1])\n",
        "        # forget gate\n",
        "        self.Wf = np.random.randn(self.in_size[2], self.out_size[2])\n",
        "        self.Uf = np.random.randn(self.out_size[2], self.out_size[2])\n",
        "        self.bf = np.zeros(self.out_size[2])\n",
        "        # input gate\n",
        "        self.Wi = np.random.randn(self.in_size[2], self.out_size[2])\n",
        "        self.Ui = np.random.randn(self.out_size[2], self.out_size[2])\n",
        "        self.bi = np.zeros(self.out_size[2])\n",
        "        # cell state\n",
        "        self.Wc = np.random.randn(self.in_size[2], self.out_size[2])\n",
        "        self.Uc = np.random.randn(self.out_size[2], self.out_size[2])\n",
        "        self.bc = np.zeros(self.out_size[2])\n",
        "        # output gate\n",
        "        self.Wo = np.random.randn(self.in_size[2], self.out_size[2])\n",
        "        self.Uo = np.random.randn(self.out_size[2], self.out_size[2])\n",
        "        self.bo = np.zeros(self.out_size[2])\n",
        "\n",
        "    def _forward_step(self, x, t):\n",
        "        x = x[:, t]\n",
        "        h_prev = self.h[:, t-1] if t > 0 else np.zeros((self.in_size[0], self.out_size[2]))\n",
        "        c_prev = self.c[:, t-1] if t > 0 else np.zeros((self.in_size[0], self.out_size[2]))\n",
        "        # forget gate\n",
        "        self.f = self.sigmoid(x @ self.Wf + h_prev @ self.Uf + self.bf)\n",
        "        # input gate\n",
        "        self.i = self.sigmoid(x @ self.Wi + h_prev @ self.Ui + self.bi)\n",
        "        # cell state\n",
        "        c_hat = self.tanh(x @ self.Wc + h_prev @ self.Uc + self.bc)\n",
        "        self.c[:, t] = self.f * c_prev + self.i * c_hat\n",
        "        # output gate\n",
        "        self.o = self.sigmoid(x @ self.Wo + h_prev @ self.Uo + self.bo)\n",
        "        # output\n",
        "        h = self.o * self.tanh(self.c[:, t])\n",
        "        return h\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.in_size = (x.shape[0], x.shape[1], x.shape[2])\n",
        "        self.x = x\n",
        "        # forward propagate through time\n",
        "        self.h = np.zeros((self.in_size[0], self.in_size[1], self.out_size[2]))\n",
        "        self.c = np.zeros((self.in_size[0], self.in_size[1], self.out_size[2]))\n",
        "        for t in range(self.in_size[1]):\n",
        "            self.h[:, t] = self._forward_step(self.x, t)\n",
        "        return self.h if self.return_sequences else self.h[:, -1:]\n",
        "\n",
        "    @classmethod\n",
        "    def _deserialize(cls, data):\n",
        "        p = data['params']\n",
        "        k = np.array(p['W_i']).shape\n",
        "        c = cls(\n",
        "            in_size=k[0],\n",
        "            units=k[1],\n",
        "            **data['meta'],\n",
        "        )\n",
        "        # set all params\n",
        "        c.compile()\n",
        "        for w in ['W', 'U', 'b']:\n",
        "            for g in ['i', 'f', 'c', 'o']:\n",
        "                wg = w+g\n",
        "                k = np.array(p[f'{w}_{g}'])\n",
        "                cur_k = getattr(c, wg)\n",
        "                assert k.shape == cur_k.shape, f\"{wg} shape mismatch: {k.shape} != {cur_k.shape}\"\n",
        "                setattr(c, wg, k)\n",
        "        return c\n",
        "\n",
        "    def _serialize(self):\n",
        "        return [{\n",
        "            'type': 'lstm',\n",
        "            'params': {\n",
        "                f'{w}_{g}': getattr(self, w+g).tolist()\n",
        "                for g in ['i', 'f', 'c', 'o']\n",
        "                for w in ['W', 'U', 'b']\n",
        "            },\n",
        "            'meta': {\n",
        "                'return_sequences': self.return_sequences,\n",
        "            }\n",
        "        }]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKjXvbgTJ4VA",
        "outputId": "2bf9a4a3-9fa0-4936-d207-6643cefa5954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 10, 5)\n",
            "[[[ 0.28654986 -0.27679391 -0.74665186 -0.00331392  0.36234761]\n",
            "  [ 0.89689543 -0.0383736   2.60044925 -0.38847812  0.00801824]\n",
            "  [ 0.13825808 -1.23614514  1.07531923  0.14212616 -0.65449765]\n",
            "  [ 0.09285611 -0.15220465  0.75606619  0.7148543   0.88647074]\n",
            "  [-0.50929909 -0.46183475 -0.65519141 -1.58960559 -0.41647173]\n",
            "  [ 0.31144274 -0.95566199  0.71868884 -0.34756257  0.07684889]\n",
            "  [-0.15792326 -1.91252661 -0.33115826  0.16559161  0.60208357]\n",
            "  [-0.10701206 -0.72534962  0.60437855  0.95402968  0.01387122]\n",
            "  [ 1.23201603 -0.18782568 -1.45886105 -0.69232699  0.83736018]\n",
            "  [ 0.4750557   1.75159701 -0.01327656 -0.7495701   0.15185625]]]\n",
            "(1, 1, 10)\n",
            "[[[-0.64211747 -0.00295774 -0.00935804 -0.22378255 -0.41902588\n",
            "    0.1151812  -0.23497289  0.00831314  0.0035991   0.18202333]]]\n"
          ]
        }
      ],
      "source": [
        "# test LSTM\n",
        "x = np.random.randn(1, 10, 5)\n",
        "print(x.shape)\n",
        "print(x)\n",
        "lstm = LSTM(in_size=x.shape[2])\n",
        "lstm.compile()\n",
        "out = lstm.forward(x)\n",
        "print(out.shape)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_JgqENyoDF7"
      },
      "source": [
        "## C. Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dSXxhuhoGQM"
      },
      "source": [
        "### 1. Sequential modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwzgEG3YoFRC"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class Sequential():\n",
        "    def __init__(self, models=None):\n",
        "        self.metrics = None\n",
        "        self.layers: list[Layer] = [] if models is None else models\n",
        "\n",
        "    def add(self, model: Layer):\n",
        "        self.layers.append(model)\n",
        "\n",
        "    def _calc_in(self):\n",
        "        last_out = None\n",
        "        for m in self.layers:\n",
        "            if hasattr(m, 'in_size') and last_out is not None:\n",
        "                # Auto feed input size from before if available\n",
        "                m.in_size = last_out\n",
        "            last_out = m.get_out_size(\n",
        "                m.in_size if last_out is None\n",
        "                else last_out\n",
        "            )\n",
        "        return last_out\n",
        "\n",
        "    def compile(self, metrics=None):\n",
        "        if metrics:\n",
        "            self.metrics = metrics\n",
        "        last_out = None\n",
        "        for m in self.layers:\n",
        "            if hasattr(m, 'in_size') and last_out is not None:\n",
        "                # Auto feed input size from before if available\n",
        "                m.in_size = last_out\n",
        "            m.compile()\n",
        "            last_out = m.get_out_size(\n",
        "                m.in_size if last_out is None\n",
        "                else last_out\n",
        "            )\n",
        "\n",
        "    def summary(self):\n",
        "        sizes = []\n",
        "        if len(self.layers) == 0:\n",
        "            print(\"No layers added\")\n",
        "            return\n",
        "        for i in range(len(self.layers)):\n",
        "            m = self.layers[i]\n",
        "            if not hasattr(m, 'in_size'):\n",
        "                in_size = sizes[i-1][2]\n",
        "            else:\n",
        "                in_size = m.in_size\n",
        "            sz = [in_size, m.get_out_size(in_size)]\n",
        "            for i in range(2):\n",
        "                if len(sz[i]) == 4:\n",
        "                    sz[i] = (None, *sz[i][1:])\n",
        "            sizes.append((m.__class__.__name__, *sz))\n",
        "\n",
        "        # describe model like keras\n",
        "        print(\n",
        "            tabulate(\n",
        "                sizes,\n",
        "                headers=['Layer Type', 'Input Shape', 'Output Shape'],\n",
        "                tablefmt='grid',\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for m in self.layers:\n",
        "            out = m.forward(out)\n",
        "        return out\n",
        "\n",
        "    def backward(self, x, y, lr=0.1):\n",
        "        yo = self.forward(x)  # 1. forward it first\n",
        "        dLdy = np.subtract(yo, y.reshape(yo.shape)) # 2. calculate loss\n",
        "        # 3. backward them from last layer till first layer\n",
        "        for i in range(len(self.layers)-1, -1, -1):\n",
        "            m = self.layers[i]\n",
        "            dLdy = m.backward(dLdy, lr)\n",
        "\n",
        "    def fit(self, x, y, epochs=2, batch_size=10, lr=0.1):\n",
        "        x0, y0 = self._batch(x, y=y, batch_size=batch_size)\n",
        "        for e in trange(epochs, desc='Fit Epoch',position=1):\n",
        "            for i in trange(len(x0), desc='Fit Batch',position=0):\n",
        "                self.backward(x0[i], y0[i], lr)\n",
        "            if self.metrics:\n",
        "                print(f\": {self.evaluate(x, y)}\")\n",
        "            print()\n",
        "\n",
        "    def _batch(self, x, batch_size, y=None):\n",
        "        n_batch = x.shape[0] // batch_size\n",
        "        bef_len = n_batch * batch_size\n",
        "        x0, y0 = [], []\n",
        "        if bef_len > 0:\n",
        "            x0 = np.split(x[:bef_len], n_batch)\n",
        "            if y is not None:\n",
        "                y0 = np.split(y[:bef_len], n_batch)\n",
        "        if x.shape[0] % batch_size != 0:\n",
        "            x0.append(x[bef_len:])\n",
        "            if y is not None:\n",
        "                y0.append(y[bef_len:])\n",
        "        return x0, y0\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        out = self.predict(x)\n",
        "        return self.metrics((out, y))\n",
        "\n",
        "    def predict(self, x):\n",
        "        # batch predict max to 50 to prevent memory error\n",
        "        batch_size = 50\n",
        "        x0, _ = self._batch(x, batch_size)\n",
        "        out = []\n",
        "        for i in trange(len(x0), desc='Predict Batch'):\n",
        "            out.append(self.forward(x0[i]))\n",
        "        return np.concatenate(out)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filename):\n",
        "        layers = [Linear, Convolution, Pooling, Flatten, Dense, LSTM, ReLU, Sigmoid, TanH]\n",
        "        with open(filename, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        m = []\n",
        "        last_out = None\n",
        "        for d in data:\n",
        "            for l in layers:\n",
        "                if l.name in d['type']:\n",
        "                    mdl = l._deserialize(defaultdict(dict, d))\n",
        "                    if hasattr(mdl, 'in_size') and last_out is not None:\n",
        "                        # Auto feed input size from before if available\n",
        "                        mdl.in_size = last_out\n",
        "                    last_out = mdl.get_out_size(\n",
        "                        mdl.in_size if last_out is None\n",
        "                        else last_out\n",
        "                    )\n",
        "                    m.append(mdl)\n",
        "                    break\n",
        "        return cls(m)\n",
        "\n",
        "    def save(self, filename):\n",
        "        data = []\n",
        "        for m in self.layers:\n",
        "            data.extend(m._serialize())\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(data, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfuEl2CxJ4VB"
      },
      "outputs": [],
      "source": [
        "def create_sequences(data, seq_length):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    data_len = len(data)\n",
        "    for i in range(data_len - seq_length):\n",
        "        seq_end = i + seq_length\n",
        "        seq_x = data[i:seq_end]\n",
        "        seq_y = data[seq_end]\n",
        "        sequences.append(seq_x)\n",
        "        targets.append(seq_y)\n",
        "    return np.array(sequences), np.array(targets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3XrEYSEoJG6"
      },
      "source": [
        "### 2. Pre-made model class (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnh60vhDfXOP"
      },
      "outputs": [],
      "source": [
        "class CNN(Sequential):\n",
        "    def __init__(self, in_size=(1,3,16,16), padding=0, n_kernels=8, kernel_size=(3,3), stride=1, units=8, pool_size=(2,2), pool_stride=2, pool_mode='avg'):\n",
        "        super().__init__([\n",
        "            Convolution(in_size=in_size, padding=padding, n_kernels=n_kernels, kernel_size=kernel_size, stride=stride),\n",
        "            Pooling(pool_size=pool_size, stride=pool_stride, mode=pool_mode),\n",
        "            Convolution(padding=padding, n_kernels=n_kernels, kernel_size=kernel_size, stride=stride),\n",
        "            Pooling(pool_size=pool_size, stride=pool_stride, mode=pool_mode),\n",
        "            Flatten(),\n",
        "            Dense(units=units, activation='relu'),\n",
        "            Dense(units=1, activation='sigmoid'),\n",
        "        ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCxyQwXqJ4VG"
      },
      "source": [
        "# Testing CNN\n",
        "Using panda or bear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbeuCE9-J4VH"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O792NmBYJ4VH"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p):\n",
        "    preds, label = p\n",
        "    return {\n",
        "        'accuracy': accuracy_score(label == 1, preds > 0.5),\n",
        "        'f1': f1_score(label == 1, preds > 0.5),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "_9vDuDv3qIo8"
      },
      "outputs": [],
      "source": [
        "data_gen_args = dict()\n",
        "datagen_train = ImageDataGenerator(validation_split=0.2, **data_gen_args)\n",
        "data_root_path = 'PandasBears'\n",
        "data_paths = [f'{data_root_path}/Train', f'{data_root_path}/Test']\n",
        "def prepare_data(data_path, datagen):\n",
        "    d = datagen.flow_from_directory(data_path, batch_size=999999999, class_mode='sparse')\n",
        "    return (d[0][0].transpose(0,3,1,2), d[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdotXqsowNmF",
        "outputId": "67d62771-3269-4410-bb06-7dc18d798703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 500 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = prepare_data(data_paths[0], datagen_train)\n",
        "X_val, y_val = prepare_data(data_paths[1], datagen_train)\n",
        "X_train = X_train / 255\n",
        "X_val = X_val / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnKWerr3J4VH"
      },
      "source": [
        "## Training Experiment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "GOytbUtPMQC-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAUzwvk5w_xj",
        "outputId": "b097e2d1-89b0-4d35-cf9a-8b06c9b62ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---------------------+---------------------+\n",
            "| Layer Type   | Input Shape         | Output Shape        |\n",
            "+==============+=====================+=====================+\n",
            "| Convolution  | (None, 3, 256, 256) | (None, 8, 256, 256) |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Pooling      | (None, 8, 256, 256) | (None, 8, 128, 128) |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Convolution  | (None, 8, 128, 128) | (None, 8, 128, 128) |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Pooling      | (None, 8, 128, 128) | (None, 8, 64, 64)   |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Flatten      | (None, 8, 64, 64)   | (None, 32768)       |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Dense        | (None, 32768)       | (None, 16)          |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Dense        | (None, 16)          | (None, 1)           |\n",
            "+--------------+---------------------+---------------------+\n"
          ]
        }
      ],
      "source": [
        "# train\n",
        "cnn = CNN(in_size=(20,3,256,256), padding=1, n_kernels=8, kernel_size=(3,3), stride=1, units=16)\n",
        "cnn.compile(metrics=compute_metrics)\n",
        "cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_siFMgAJ4VH",
        "outputId": "44324cdb-e478-431a-ef63-8a5a30ee4f3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fit Batch: 100%|██████████| 50/50 [04:39<00:00,  5.58s/it]\n",
            "Predict Batch: 100%|██████████| 10/10 [01:06<00:00,  6.64s/it]\n",
            "\n",
            "Fit Epoch:  50%|█████     | 1/2 [05:45<05:45, 345.63s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": {'accuracy': 0.974, 'f1': 0.9735234215885946}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fit Batch: 100%|██████████| 50/50 [04:58<00:00,  5.98s/it]\n",
            "Predict Batch: 100%|██████████| 10/10 [01:19<00:00,  7.91s/it]\n",
            "\n",
            "Fit Epoch: 100%|██████████| 2/2 [12:03<00:00, 361.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": {'accuracy': 0.98, 'f1': 0.9797570850202428}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "cnn.fit(X_train, y_train, batch_size=10, epochs=2, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.save('test_cnn.json')"
      ],
      "metadata": {
        "id": "588tGq_MMDlD"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict"
      ],
      "metadata": {
        "id": "INDumS9EL5C7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "XtPYB03QJ4VI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c0e1067-4cb6-4dcd-f9dd-cf0847a73e86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predict Batch: 100%|██████████| 2/2 [00:16<00:00,  8.04s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.96, 'f1': 0.9591836734693877}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "compute_metrics((cnn.predict(X_val), y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Predict Saved Model"
      ],
      "metadata": {
        "id": "jqYxZkvBL8Tp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "LqUBc9D9J4VI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faacd0b1-d814-4680-d04c-b5393584c28a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---------------------+---------------------+\n",
            "| Layer Type   | Input Shape         | Output Shape        |\n",
            "+==============+=====================+=====================+\n",
            "| Convolution  | (None, 3, 256, 256) | (None, 8, 256, 256) |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Pooling      | (None, 8, 256, 256) | (None, 8, 128, 128) |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Convolution  | (None, 8, 128, 128) | (None, 8, 128, 128) |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Pooling      | (None, 8, 128, 128) | (None, 8, 64, 64)   |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Flatten      | (None, 8, 64, 64)   | (None, 32768)       |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Dense        | (None, 32768)       | (None, 16)          |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Dense        | (None, 16)          | (None, 1)           |\n",
            "+--------------+---------------------+---------------------+\n"
          ]
        }
      ],
      "source": [
        "m = Sequential.load('test_cnn.json')\n",
        "m.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "wM6K47ukJ4VI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "809e0fd5-748f-4fa5-e3e1-57c60dbabae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predict Batch: 100%|██████████| 2/2 [00:17<00:00,  8.62s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.96, 'f1': 0.9591836734693877}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "compute_metrics((m.predict(X_val), y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgYm3qPaJ4VI"
      },
      "source": [
        "## Training Experiment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "dKAix2RhMlgU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKflyfV5J4VI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65274173-7480-4796-b8e5-a9f70a8d3402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---------------------+---------------------+\n",
            "| Layer Type   | Input Shape         | Output Shape        |\n",
            "+==============+=====================+=====================+\n",
            "| Convolution  | (None, 3, 256, 256) | (None, 8, 256, 256) |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Pooling      | (None, 8, 256, 256) | (None, 8, 128, 128) |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Convolution  | (None, 8, 128, 128) | (None, 6, 128, 128) |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Pooling      | (None, 6, 128, 128) | (None, 6, 64, 64)   |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Flatten      | (None, 6, 64, 64)   | (None, 24576)       |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Dense        | (None, 24576)       | (None, 16)          |\n",
            "+--------------+---------------------+---------------------+\n",
            "| Dense        | (None, 16)          | (None, 1)           |\n",
            "+--------------+---------------------+---------------------+\n"
          ]
        }
      ],
      "source": [
        "# train a different version\n",
        "cnn2 = Sequential([\n",
        "    Convolution(in_size=(20,3,256,256), padding=2, n_kernels=8, kernel_size=(5,5), stride=1),\n",
        "    Pooling(mode='max'),\n",
        "    Convolution(padding=1, n_kernels=6, kernel_size=(3,3), stride=1),\n",
        "    Pooling(mode='avg'),\n",
        "    Flatten(),\n",
        "    Dense(units=16, activation='relu'),\n",
        "    Dense(units=1, activation='sigmoid'),\n",
        "])\n",
        "cnn2.compile(metrics=compute_metrics)\n",
        "cnn2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxocp7mFJ4VI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c1e4010-b0df-4cfc-b348-15ab5d807f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fit Batch: 100%|██████████| 50/50 [08:49<00:00, 10.59s/it]\n",
            "Predict Batch: 100%|██████████| 10/10 [02:14<00:00, 13.49s/it]\n",
            "\n",
            "Fit Epoch:  50%|█████     | 1/2 [11:04<11:04, 664.43s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": {'accuracy': 0.958, 'f1': 0.9565217391304348}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fit Batch: 100%|██████████| 50/50 [08:42<00:00, 10.46s/it]\n",
            "Predict Batch: 100%|██████████| 10/10 [02:13<00:00, 13.36s/it]\n",
            "\n",
            "Fit Epoch: 100%|██████████| 2/2 [22:00<00:00, 660.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": {'accuracy': 0.862, 'f1': 0.8770053475935828}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "cnn2.fit(X_train, y_train, batch_size=10, epochs=2, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEjbVx1AJ4VI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2ed3c8-ee73-4681-8902-02c518e5dd7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predict Batch: 100%|██████████| 10/10 [02:12<00:00, 13.22s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.862, 'f1': 0.8770053475935828}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "compute_metrics((cnn2.predict(X_train), y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwI8mPmTJ4VJ"
      },
      "source": [
        "## Loading Model Spesifikasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7VpNJg8J4VJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dadcd502-92fe-4423-95a7-161d04eaa478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-------------------+-------------------+\n",
            "| Layer Type   | Input Shape       | Output Shape      |\n",
            "+==============+===================+===================+\n",
            "| Convolution  | (None, 3, 16, 16) | (None, 8, 14, 14) |\n",
            "+--------------+-------------------+-------------------+\n",
            "| Pooling      | (None, 8, 14, 14) | (None, 8, 7, 7)   |\n",
            "+--------------+-------------------+-------------------+\n",
            "| Flatten      | (None, 8, 7, 7)   | (None, 392)       |\n",
            "+--------------+-------------------+-------------------+\n",
            "| Dense        | (None, 392)       | (None, 8)         |\n",
            "+--------------+-------------------+-------------------+\n",
            "| Dense        | (None, 8)         | (None, 1)         |\n",
            "+--------------+-------------------+-------------------+\n"
          ]
        }
      ],
      "source": [
        "m = Sequential.load('model_spek_cnn.json')\n",
        "m.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfjZXB2TJ4VJ"
      },
      "outputs": [],
      "source": [
        "p = np.random.randn(\n",
        "    5, 3, 16, 16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HblB5UX1J4VJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8aaffa0-1fc3-4812-dbc8-b81741ce584b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predict Batch: 100%|██████████| 1/1 [00:00<00:00, 87.38it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.54118097],\n",
              "       [0.5517079 ],\n",
              "       [0.5282708 ],\n",
              "       [0.49202831],\n",
              "       [0.52122151]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "m.predict(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Testing"
      ],
      "metadata": {
        "id": "SKHN5lBLLmRl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYXlFes8J4VJ"
      },
      "source": [
        "## RNN Forward Pass Testing + Save/Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSBoJmnUJ4VJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf2c981-1ba0-4703-8a22-1a2472832dbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------------+----------------+\n",
            "| Layer Type   | Input Shape     | Output Shape   |\n",
            "+==============+=================+================+\n",
            "| LSTM         | (None, None, 5) | (None, 1, 10)  |\n",
            "+--------------+-----------------+----------------+\n",
            "| Dense        | (None, 1, 10)   | (None, 1)      |\n",
            "+--------------+-----------------+----------------+\n"
          ]
        }
      ],
      "source": [
        "rnn = Sequential([\n",
        "    LSTM(in_size=5, units=10),\n",
        "    Dense(inputs=10, units=1, activation='sigmoid')\n",
        "])\n",
        "rnn.compile()\n",
        "rnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY4wiNRBJ4VJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b79b87-b650-4dc2-e453-3dce966c4573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 10, 5)\n",
            "[[[ 1.67815934  0.95695283  0.27787654 -0.39265758  0.66191067]\n",
            "  [-1.25531444 -0.42498035  0.21000236 -1.13629448  2.3582527 ]\n",
            "  [-0.52725563 -0.61988274  0.12180147 -1.21837795 -1.36166998]\n",
            "  [ 0.49656554 -1.23067168 -0.80301389 -0.9898318  -0.58856035]\n",
            "  [-0.53006427  0.76842053 -0.37384234  0.125754    1.18966862]\n",
            "  [-0.23673754  1.36260388 -0.34547994  0.41988483 -2.4991309 ]\n",
            "  [ 0.50318535  1.75929159  0.12941912  1.91642448  0.0920533 ]\n",
            "  [ 0.91613984 -1.7687264  -0.02425373  0.43317379  0.28299487]\n",
            "  [-0.11017796  0.14351769  0.35156993  1.47157511  0.48471243]\n",
            "  [ 0.48728174 -0.59976945 -0.57598336  0.48365966  0.47992505]]]\n"
          ]
        }
      ],
      "source": [
        "# prepare data\n",
        "x = np.random.randn(1, 10, 5)\n",
        "print(x.shape)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test forward\n",
        "out = rnn.forward(x)\n",
        "print(out.shape)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "S9OFrAppW5Gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf6ad1b-46d5-4aee-f176-08f9976b1692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1, 1)\n",
            "[[[0.46909752]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "rnn.save('test_rnn.json')"
      ],
      "metadata": {
        "id": "-nYikY84Weqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test forward saved model\n",
        "r = Sequential.load('test_rnn.json')\n",
        "out = r.forward(x)\n",
        "print(out.shape)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "fRZs6TolWjyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7370f232-c6ef-4c08-f00f-23738101068f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1, 1)\n",
            "[[[0.46909752]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTFtUou2J4VK"
      },
      "source": [
        "## RNN BMRA Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJno2qG0J4VK"
      },
      "outputs": [],
      "source": [
        "# test on BMRA data\n",
        "import pandas as pd\n",
        "bmra = pd.read_csv('Test_stock_market.csv')\n",
        "# use only high, low, volum, open and close as features\n",
        "bmra = bmra[['High', 'Low', 'Volume', 'Open', 'Close']]\n",
        "bmra = bmra.to_numpy()\n",
        "\n",
        "def create_ds(seq_len=10):\n",
        "  x_test, y_test = create_sequences(bmra, seq_len)\n",
        "  y_test = y_test.reshape(y_test.shape[0], 1, y_test.shape[1])\n",
        "  return x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8Re1Sw5J4VK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d387c340-9cf7-469b-f671-e3174bc363b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------------+----------------+\n",
            "| Layer Type   | Input Shape     | Output Shape   |\n",
            "+==============+=================+================+\n",
            "| LSTM         | (None, None, 5) | (None, 1, 32)  |\n",
            "+--------------+-----------------+----------------+\n",
            "| Dense        | (None, 1, 32)   | (None, 5)      |\n",
            "+--------------+-----------------+----------------+\n",
            "| Linear       | (None, 5)       | (None, 5)      |\n",
            "+--------------+-----------------+----------------+\n"
          ]
        }
      ],
      "source": [
        "rnn = Sequential([\n",
        "    LSTM(5, 32),\n",
        "    Dense(32, 5, 'relu'),\n",
        "    Linear(5, 5),\n",
        "])\n",
        "rnn.compile()\n",
        "rnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp4ZbJApJ4VK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec46c26b-1f1d-48e8-bd94-5209990cde1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Testing forward for sequence length 5 ====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predict Batch:   0%|          | 0/1 [00:00<?, ?it/s]<ipython-input-9-192e97f2abc8>:5: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n",
            "Predict Batch: 100%|██████████| 1/1 [00:00<00:00, 45.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input 1: [[3.49e+00 3.38e+00 3.84e+04 3.48e+00 3.43e+00]\n",
            " [3.58e+00 3.43e+00 1.32e+04 3.57e+00 3.56e+00]\n",
            " [3.51e+00 3.40e+00 2.54e+04 3.51e+00 3.45e+00]\n",
            " [3.49e+00 3.32e+00 2.80e+04 3.43e+00 3.36e+00]\n",
            " [3.46e+00 3.33e+00 4.16e+04 3.33e+00 3.40e+00]]\n",
            "Output 1: [[ 0.00717982  0.00927167 -0.00255867 -0.00687925 -0.00027295]]\n",
            "Target 1: [[3.54e+00 3.43e+00 8.40e+03 3.46e+00 3.54e+00]]\n",
            "Overall MSE Loss: 187001092.91319346\n",
            "==== Testing forward for sequence length 10 ====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predict Batch: 100%|██████████| 1/1 [00:00<00:00, 45.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input 1: [[3.49e+00 3.38e+00 3.84e+04 3.48e+00 3.43e+00]\n",
            " [3.58e+00 3.43e+00 1.32e+04 3.57e+00 3.56e+00]\n",
            " [3.51e+00 3.40e+00 2.54e+04 3.51e+00 3.45e+00]\n",
            " [3.49e+00 3.32e+00 2.80e+04 3.43e+00 3.36e+00]\n",
            " [3.46e+00 3.33e+00 4.16e+04 3.33e+00 3.40e+00]\n",
            " [3.54e+00 3.43e+00 8.40e+03 3.46e+00 3.54e+00]\n",
            " [3.54e+00 3.04e+00 7.98e+04 3.54e+00 3.05e+00]\n",
            " [3.27e+00 2.95e+00 4.57e+04 3.12e+00 2.95e+00]\n",
            " [3.20e+00 2.95e+00 4.28e+04 3.10e+00 3.00e+00]\n",
            " [3.08e+00 2.93e+00 3.30e+04 2.98e+00 2.96e+00]]\n",
            "Output 1: [[ 0.00718038  0.00927147 -0.00255806 -0.00687917 -0.00027303]]\n",
            "Target 1: [[3.04e+00 2.99e+00 2.49e+04 3.00e+00 2.99e+00]]\n",
            "Overall MSE Loss: 140291410.46944797\n",
            "==== Testing forward for sequence length 15 ====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predict Batch: 100%|██████████| 1/1 [00:00<00:00, 24.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input 1: [[3.49e+00 3.38e+00 3.84e+04 3.48e+00 3.43e+00]\n",
            " [3.58e+00 3.43e+00 1.32e+04 3.57e+00 3.56e+00]\n",
            " [3.51e+00 3.40e+00 2.54e+04 3.51e+00 3.45e+00]\n",
            " [3.49e+00 3.32e+00 2.80e+04 3.43e+00 3.36e+00]\n",
            " [3.46e+00 3.33e+00 4.16e+04 3.33e+00 3.40e+00]\n",
            " [3.54e+00 3.43e+00 8.40e+03 3.46e+00 3.54e+00]\n",
            " [3.54e+00 3.04e+00 7.98e+04 3.54e+00 3.05e+00]\n",
            " [3.27e+00 2.95e+00 4.57e+04 3.12e+00 2.95e+00]\n",
            " [3.20e+00 2.95e+00 4.28e+04 3.10e+00 3.00e+00]\n",
            " [3.08e+00 2.93e+00 3.30e+04 2.98e+00 2.96e+00]\n",
            " [3.04e+00 2.99e+00 2.49e+04 3.00e+00 2.99e+00]\n",
            " [3.03e+00 2.97e+00 2.90e+03 3.01e+00 3.00e+00]\n",
            " [3.00e+00 2.90e+00 1.49e+04 3.00e+00 2.90e+00]\n",
            " [2.93e+00 2.84e+00 2.39e+04 2.93e+00 2.87e+00]\n",
            " [2.85e+00 2.80e+00 2.90e+04 2.82e+00 2.80e+00]]\n",
            "Output 1: [[ 0.00718038  0.00927147 -0.00255806 -0.00687917 -0.00027303]]\n",
            "Target 1: [[2.92e+00 2.70e+00 4.30e+04 2.70e+00 2.92e+00]]\n",
            "Overall MSE Loss: 150663448.80840382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# test forward and MSE loss\n",
        "for seq_len in [5, 10, 15]:\n",
        "  print('==== Testing forward for sequence length', seq_len, '====')\n",
        "  x_test, y_test = create_ds(seq_len)\n",
        "  y_pred = rnn.predict(x_test)\n",
        "  loss = np.mean(np.square(y_pred - y_test))\n",
        "  print(\"Input 1:\", x_test[0])\n",
        "  print(\"Output 1:\", y_pred[0])\n",
        "  print(\"Target 1:\", y_test[0])\n",
        "  print(\"Overall MSE Loss:\", loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amDTiYMOJ4VK"
      },
      "source": [
        "## Loading RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCTFUUX2J4VK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb44883e-7893-4119-ca62-b772ac39f696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------------+----------------+\n",
            "| Layer Type   | Input Shape     | Output Shape   |\n",
            "+==============+=================+================+\n",
            "| LSTM         | (None, None, 5) | (None, 1, 64)  |\n",
            "+--------------+-----------------+----------------+\n",
            "| Dense        | (None, 1, 64)   | (None, 5)      |\n",
            "+--------------+-----------------+----------------+\n"
          ]
        }
      ],
      "source": [
        "s = Sequential.load('model_spek_rnn.json')\n",
        "s.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL9fJGs4J4VK"
      },
      "source": [
        "# Save and Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iwZSh4tJ4VK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c4e8c9-6a1e-468f-ed14-e2cfe67b5471"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((455, 30), (114, 30), (455,), (114,))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "d = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(d[0], d[1], test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xZy0klDJ4VL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d387fc-301b-4444-b712-66dee791ed38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---------------+----------------+\n",
            "| Layer Type   | Input Shape   | Output Shape   |\n",
            "+==============+===============+================+\n",
            "| Dense        | (None, 30)    | (None, 9)      |\n",
            "+--------------+---------------+----------------+\n",
            "| Dense        | (None, 9)     | (None, 1)      |\n",
            "+--------------+---------------+----------------+\n"
          ]
        }
      ],
      "source": [
        "m = Sequential([\n",
        "    Dense(X_train.shape[1], 9, 'relu'),\n",
        "    Dense(0, 1, 'sigmoid'),\n",
        "])\n",
        "m.compile(metrics=compute_metrics)\n",
        "m.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYBxOImWJ4VL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f1eca51-5dfb-498c-bd24-f662cf36b25e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fit Batch: 100%|██████████| 5/5 [00:00<00:00, 987.87it/s]\n",
            "Predict Batch: 100%|██████████| 10/10 [00:00<00:00, 12554.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": {'accuracy': 0.9714285714285714, 'f1': 0.977469670710572}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fit Batch: 100%|██████████| 5/5 [00:00<00:00, 3402.81it/s]\n",
            "Predict Batch: 100%|██████████| 10/10 [00:00<00:00, 14098.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": {'accuracy': 0.9802197802197802, 'f1': 0.9843478260869565}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fit Batch: 100%|██████████| 5/5 [00:00<00:00, 2856.38it/s]\n",
            "Predict Batch: 100%|██████████| 10/10 [00:00<00:00, 11841.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": {'accuracy': 0.9802197802197802, 'f1': 0.9843478260869565}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fit Batch: 100%|██████████| 5/5 [00:00<00:00, 3356.52it/s]\n",
            "Predict Batch: 100%|██████████| 10/10 [00:00<00:00, 12854.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": {'accuracy': 0.9824175824175824, 'f1': 0.986111111111111}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fit Batch: 100%|██████████| 5/5 [00:00<00:00, 3468.09it/s]\n",
            "Predict Batch: 100%|██████████| 10/10 [00:00<00:00, 14433.26it/s]\n",
            "Fit Epoch: 100%|██████████| 5/5 [00:00<00:00, 55.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": {'accuracy': 0.9802197802197802, 'f1': 0.9843478260869565}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "m.fit(X_train, Y_train, epochs=5, lr=0.1, batch_size=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z36SPCSaJ4VL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e733cad-5e43-44b8-e871-185f222792fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predict Batch: 100%|██████████| 3/3 [00:00<00:00, 7153.45it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9912280701754386, 'f1': 0.993006993006993}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "compute_metrics((m.predict(X_test), Y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3FLrjzoJ4VL"
      },
      "outputs": [],
      "source": [
        "m.save(\"test1.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l033A9_EJ4VL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba8aa15-6f6e-45d9-f379-19fd7c29a609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---------------+----------------+\n",
            "| Layer Type   | Input Shape   | Output Shape   |\n",
            "+==============+===============+================+\n",
            "| Dense        | (None, 30)    | (None, 9)      |\n",
            "+--------------+---------------+----------------+\n",
            "| Dense        | (None, 9)     | (None, 1)      |\n",
            "+--------------+---------------+----------------+\n"
          ]
        }
      ],
      "source": [
        "n = Sequential.load(\"test1.json\")\n",
        "n.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_j55pZNJ4VL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ede7596-fd49-46bc-ee76-a0abc5a696bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predict Batch: 100%|██████████| 3/3 [00:00<00:00, 5506.74it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9912280701754386, 'f1': 0.993006993006993}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "compute_metrics((n.predict(X_test), Y_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}